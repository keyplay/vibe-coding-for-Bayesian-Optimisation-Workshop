{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702cc469-e6ac-43fc-abec-7e3082aec6b2",
   "metadata": {},
   "source": [
    "### Optimisation of a non-convex 2-D function with Bayesian Optimisation\n",
    "\n",
    "#### Hackathon Brief\n",
    "This hackathon involves the optimisation of a complex, non-convex function with Bayesian Optimisation. \n",
    "\n",
    "#### Optimisation Details and Constraints\n",
    "Using your knowledge of how Gaussian Processes and Bayesian Optimisation operate (mean function, convariance/kernal functions, initialisation points, acquisition functions etc.), your task is to develop two python classes: a GP class and a BO class (similar to what was seen previously in sections B and C) to obtain input values (x1 and x2) which conresponds to the minima of a complex, non-convex 2D function. The exact function is not revealed, but is based on a modification of either the `Rastrigin, Ackley or Shubert function`. <a href=\"https://www.sfu.ca/~ssurjano/optimization.html\"> See here for equations.</a>\n",
    "\n",
    "```\n",
    "Optimisation Task : Minimisation.\n",
    "Search space      : x1 and x2 both from -10 to 10. \n",
    "Constrains        : Budget of 50 iterations. (See guidance for advanced teams.)\n",
    "Training points   : Allowed a maximum of 15 initial training points.\n",
    "\n",
    "```\n",
    "\n",
    "#### Submission Details\n",
    "A template and example of a GP and BO class function can be seen below (the same code structure as seen previously). You are allowed to write your own BO class or copy-paste/make modifications to any of the previously seen BO classes. \n",
    "\n",
    "You must include the attributes `self.X` and `self.Y` corresponding to all of your evaluated inputs and outputs in your BO class as this will be used to retrive the information used for scoring. You must include the input `objective_func` to your BO class as the intructor will parse the scoring objecting function as as input! Please remove/comment your own test objective functions when submitting!\n",
    "\n",
    "```python\n",
    "#submission should look something like the following\n",
    "class GP: #if you have any separate classes other than the BO class\n",
    "    def __init__(self, ...):\n",
    "        ...\n",
    "#BO class\n",
    "class BO: \n",
    "    def __init__(self, ...):\n",
    "        self.X = #training data which the evaluated data is to be appended\n",
    "        self.Y = #evaluated via the objective function using self.X\n",
    "\n",
    "# BO Execution Block\n",
    "X_training = [...] #maximum of 15 training points\n",
    "\n",
    "\n",
    "#Please remove/comment your objective functions when submitting!\n",
    "#def obj_func(X):\n",
    "#\treturn (...)\n",
    "\n",
    "BO_m = BO(...,\n",
    "          objective_func = obj_func, #please have objective_func as input and do not change the 'obj_func' variable name!\n",
    "         ...)\n",
    "```\n",
    "\n",
    "Once completed, please upload your classes to the Stremlet submission page where your code will be tested against an objective function. The scoring is based on the lowest output obtained (the lower the \n",
    "\n",
    "#### Guidance (Advanced):\n",
    "It is encouraged that you write your own GP/BO algorithm! You have a range of possibilities from implimenting better kernels to using designer acquisition functions (given some knowledge of the function). There are packages (see below) that can be used to drastically improve tensor manipulation and increase the speed to which your code runs. Please be adviced that the code will be run on the instructor's laptop and will not have any GPUs!\n",
    "\n",
    "You do not have a maximum iteration budget. However, your score will be penalised by the number of iterations^2! (ie, your final score will be `min(self.Y) - iteration**2`) With this, you *must* define the attribute `self.iterations = # number_of_iterations` in your BO class.\n",
    "\n",
    "#### Guidance (Intermediates):\n",
    "You are more than welcomed to write your own GP/BO algorithm! You can also use the template given below. You should have some familiarity with how the basic GP and BO class in the given template work from sections B and C. \n",
    "\n",
    "The search space is defined in the BO class already. You can change the number of points per variable `number_points_pervariable = 200` within the BO class if you wish. Here are some additional guidance:\n",
    "\n",
    "##### Form of Training Points Input (X):\n",
    "If you are using the template given, like previous examples, the input of training points X must be a matrix with shape (N,2) where N is the number of training points. An example of a symmetrical input can be made with list comprehension. Tailored matrixes can be made and inputed directly. Example:\n",
    "\n",
    "``` python\n",
    "x1loc  = [1,2]; x2loc = [3,4]\n",
    "X_training = np.array([[x1,x2] for x1 in x1loc for x2 in x2loc])\n",
    "\n",
    "# This is equivalent to:\n",
    "X_training =  [[1, 3], [1, 4], [2, 3], [2, 4]]\n",
    "```\n",
    "\n",
    "##### Form of GP and BO class\n",
    "It is advised that key aspects of the class (ex. hyperparameters, acquisition function etc.) be observed and modified first to obtain a better BO algorithm prior to changing the mathematical implimentation of the GP/BO class. Under the BO class, you must remember to include (or not delete) the addition of the new training data point to 'self.X' as the score ranking will be dependant on observing which inputs in self.X corresponds to the lowest output obtained in self.Y.\n",
    "\n",
    "##### Some Starting Points to Help\n",
    "1. The potential objective function is given - look up the functions! You can plot these functions (or observe them through a graphing website like Desmos) to observe the nature of the functions. (symmetry, number of minima etc.) Use this information to decide your BO strategy.\n",
    "2. Similar to Section C, modify the code to include plots of how your BO functions are performing over each iteration. This will help you visualise and evaluate the usefulness of your modifications.\n",
    "3. Decide the initial training points (Random? Bias? Uniform distrubution?) and acquisition function (Greedy? Purely explorative? Lower confidence bounnd? Expected Improvement?) with the associated hyperparameters.\n",
    "\n",
    "Other considerations:\n",
    "1. Will changing the GP mean function help with efficiency?\n",
    "2. Are there better kernels for non-convex functions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3508c-2a42-4f86-94ae-c9934914bc66",
   "metadata": {},
   "source": [
    "#### Package Imports\n",
    "\n",
    "Packages are limited to the the ones listed in the package cell - Talk to one of the intructors to ask if it is possible to import other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2102cb-a585-4155-9b68-1316979f58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using google collab, run the following pip installs!\n",
    "!pip install sobol_seq\n",
    "!pip install plotly\n",
    "!pip install gpytorch\n",
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd81d8-b013-4580-8ef8-35290937dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "import plotly.graph_objs as go\n",
    "from scipy.integrate import quad\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import minimize, differential_evolution, NonlinearConstraint\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import time\n",
    "import sobol_seq\n",
    "import torch\n",
    "import gpytorch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870b6f0-ec81-4323-b67e-b90187ac44f0",
   "metadata": {},
   "source": [
    "#### GP class, BO class, BO Execution Template/Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a4217-7d4d-4528-8216-62691feb9921",
   "metadata": {},
   "source": [
    "```python\n",
    "#GP class\n",
    "class GP:\n",
    "    def __init__(self, X, Y, kernel):\n",
    "        \n",
    "        self.X, self.Y, self.kernel                                  = X, Y, kernel\n",
    "        self.number_of_point, self.nx_dimensions, self.ny_dimensions = X.shape[0], X.shape[1], Y.shape[1]\n",
    "        self.multistart_loops                                        = 3\n",
    "\n",
    "        self.X_mean, self.X_std     = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "        self.Y_mean, self.Y_std     = np.mean(Y, axis=0), np.std(Y, axis=0)\n",
    "        self.X_norm, self.Y_norm    = (X-self.X_mean)/self.X_std, (Y-self.Y_mean)/self.Y_std\n",
    "\n",
    "        self.hyperparam_optimized , self.inverse_covariance_matrix_opt   = self.determine_hyperparameters()     \n",
    "        \n",
    "    def Cov_mat(self, kernel, X_norm, W, sf2):\n",
    "        if kernel == 'SquaredExponential':\n",
    "            xixj_euclidean_distance = cdist(X_norm, X_norm, 'seuclidean', V=W)**2 \n",
    "            cov_matrix = sf2*np.exp(-0.5*xixj_euclidean_distance)\n",
    "            return (cov_matrix)\n",
    "        else:\n",
    "            print('ERROR no kernel with name ', kernel)\n",
    "\n",
    "    def negative_loglikelihood(self, hyper, X, Y):\n",
    "        n_point, nx_dim = self.number_of_point, self.nx_dimensions\n",
    "        kernel          = self.kernel\n",
    "        \n",
    "        W               = np.exp(2*hyper[:nx_dim])   \n",
    "        sf2             = np.exp(2*hyper[nx_dim])    \n",
    "        sn2             = np.exp(2*hyper[nx_dim+1])  \n",
    "\n",
    "        K       = self.Cov_mat(kernel, X, W, sf2)  \n",
    "        K       = K + (sn2 + 1e-8)*np.eye(n_point)\n",
    "        K       = (K + K.T)*0.5                    \n",
    "        L       = np.linalg.cholesky(K)           \n",
    "        logdetK = 2 * np.sum(np.log(np.diag(L)))   \n",
    "        invLY   = np.linalg.solve(L,Y)             \n",
    "        alpha   = np.linalg.solve(L.T,invLY)       \n",
    "        NLL     = np.dot(Y.T,alpha) + logdetK      \n",
    "        return (NLL)\n",
    "\n",
    "    def determine_hyperparameters(self): \n",
    "        lower_bound = np.array([-4.]*(self.nx_dimensions+1) + [-8.])  \n",
    "        upper_bound = np.array([4.]*(self.nx_dimensions+1) + [ -2.]) \n",
    "        bounds      = np.hstack((lower_bound.reshape(self.nx_dimensions+2,1), upper_bound.reshape(self.nx_dimensions+2,1)))\n",
    "    \n",
    "        multi_startvec                = sobol_seq.i4_sobol_generate(self.nx_dimensions + 2, self.multistart_loops)\n",
    "        \n",
    "        temp_min_hyperparams          = [0.]*self.multistart_loops\n",
    "        temp_loglikelihood            = np.zeros((self.multistart_loops))\n",
    "        hyperparam_optimized          = np.zeros((self.nx_dimensions+2, self.ny_dimensions)) #for best solutions\n",
    "        inverse_covariance_matrix_opt = []\n",
    "        \n",
    "        for i in range(self.ny_dimensions):\n",
    "            for j in range(self.multistart_loops ):\n",
    "                hyperparams_initialisation   = lower_bound + (upper_bound-lower_bound)*multi_startvec[j,:] # mapping sobol unit cube to boudns\n",
    "                result  = minimize(self.negative_loglikelihood,\n",
    "                                   hyperparams_initialisation,\n",
    "                                   args     = (self.X_norm, self.Y_norm[:,i]),\n",
    "                                   method   = 'SLSQP',\n",
    "                                   options  = {'disp':False,'maxiter':10000},\n",
    "                                   bounds   = bounds,\n",
    "                                   tol      = 1e-12)\n",
    "                temp_min_hyperparams[j] = result.x\n",
    "                temp_loglikelihood[j]   = result.fun  \n",
    "\n",
    "            minimumloglikelihood_index    = np.argmin(temp_loglikelihood)\n",
    "            hyperparam_optimized[:,i]     = temp_min_hyperparams[minimumloglikelihood_index  ]\n",
    "    \n",
    "            lengthscale_opt         = np.exp(2.*hyperparam_optimized[:self.nx_dimensions,i])\n",
    "            signalvarience_opt      = np.exp(2.*hyperparam_optimized[self.nx_dimensions,i])\n",
    "            noise_opt               = np.exp(2.*hyperparam_optimized[self.nx_dimensions+1,i]) + 1e-8\n",
    "    \n",
    "            covarience_matrix_opt              = self.Cov_mat(self.kernel, self.X_norm, lengthscale_opt,signalvarience_opt) + noise_opt*np.eye(self.number_of_point)   \n",
    "            inverse_covariance_matrix_opt     += [np.linalg.solve(covarience_matrix_opt, np.eye(self.number_of_point))]\n",
    "        return (hyperparam_optimized , inverse_covariance_matrix_opt)\n",
    "\n",
    "\n",
    "    def calc_cov_sample(self,xnorm,Xnorm,ell,sf2):\n",
    "        nx_dim     = self.nx_dimensions\n",
    "        dist       = cdist(Xnorm, xnorm.reshape(1,nx_dim), 'seuclidean', V=ell)**2\n",
    "        cov_matrix = sf2 * np.exp(-.5*dist)\n",
    "        return (cov_matrix )         \n",
    "\n",
    "\n",
    "    def GP_inference_np(self, x):\n",
    "        nx_dim                   = self.nx_dimensions\n",
    "        kernel, ny_dim           = self.kernel, self.ny_dimensions\n",
    "        hypopt, Cov_mat          = self.hyperparam_optimized, self.Cov_mat\n",
    "        stdX, stdY, meanX, meanY = self.X_std, self.Y_std, self.X_mean, self.Y_mean\n",
    "        calc_cov_sample          = self.calc_cov_sample\n",
    "        invKsample               = self.inverse_covariance_matrix_opt\n",
    "        Xsample, Ysample         = self.X_norm, self.Y_norm\n",
    "\n",
    "        xnorm = (x - meanX)/stdX\n",
    "        mean  = np.zeros(ny_dim)\n",
    "        var   = np.zeros(ny_dim)\n",
    "        \n",
    "        for i in range(ny_dim):\n",
    "            invK           = invKsample[i]\n",
    "            hyper          = hypopt[:,i]\n",
    "            ellopt, sf2opt = np.exp(2*hyper[:nx_dim]), np.exp(2*hyper[nx_dim])\n",
    "\n",
    "            k             = calc_cov_sample(xnorm,Xsample,ellopt,sf2opt)\n",
    "            raw_mean      = np.matmul(np.matmul(k.T,invK),Ysample[:,i]).item()\n",
    "            mean[i]       = raw_mean\n",
    "            raw_var_array = np.maximum(0, sf2opt - (k.T @ invK @ k)).item()\n",
    "            raw_var       = raw_var_array\n",
    "            var[i]        = raw_var\n",
    "    \n",
    "        mean_sample = mean*stdY + meanY\n",
    "        var_sample  = var*stdY**2\n",
    "        \n",
    "        return (mean_sample, var_sample)\n",
    "\n",
    "#BO class\n",
    "class BO: \n",
    "    def __init__(self, X, kernel, acquisition_function, objective_func, acquisition_hyperparam, iterations):       \n",
    "        number_points_pervariable      = 200\n",
    "        number_points_searchspace      = number_points_pervariable ** (np.shape(X_training)[1])\n",
    "        X_searchspace                  = np.linspace(-10, 10, num=number_points_pervariable)\n",
    "        X_searchspace                  = np.array([[x,y] for x in X_searchspace for y in X_searchspace])\n",
    "        \n",
    "        self.X, self.iterations  = X, iterations\n",
    "        Fx_training              = np.array([objective_func(x) for x in X_training])\n",
    "        self.Y                   = Fx_training.reshape(Fx_training.shape[0],1)\n",
    "        \n",
    "        fx_searchspace           = np.array([objective_func(x) for x in X_searchspace])\n",
    "        n_candidates             = X_searchspace.shape[0]\n",
    "        Ysearchspace_mean        = np.zeros(n_candidates)\n",
    "        Ysearchspace_std         = np.zeros(n_candidates)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            GP_m = GP(self.X, self.Y, kernel)\n",
    "            \n",
    "            for number in range(len(X_searchspace)):\n",
    "                m_ii, std_ii   = GP_m.GP_inference_np(X_searchspace[number])\n",
    "                Ysearchspace_mean[number] = m_ii.item()\n",
    "                Ysearchspace_std[number]  = std_ii.item()\n",
    "   \n",
    "            if acquisition_function == 'greedy':\n",
    "                X_acquisitionfunc = self.greedy_fullexplotative(X_searchspace,Ysearchspace_mean, Ysearchspace_std )     \n",
    "            else: \n",
    "                print('No acquisition function called ', acquisition_function)\n",
    "                break\n",
    "            \n",
    "            self.X = np.append(self.X, [X_acquisitionfunc],0)\n",
    "            self.Y = np.append(self.Y, [[objective_func(X_acquisitionfunc)]],0)\n",
    "    \n",
    "    def greedy_fullexplotative(self, X_searchspace, Ysearchspace_mean, Ysearchspace_std):\n",
    "        return (X_searchspace[np.argmin(Ysearchspace_mean)])\n",
    "\n",
    "# BO Execution Block\n",
    "X_training = [...]\n",
    "\n",
    "#Please remove/comment your objective functions when submitting!\n",
    "#def obj_func(X):\n",
    "#\treturn (...)\n",
    "\n",
    "BO_m = BO(X = ...,  \n",
    "           kernel = ..., \n",
    "           iterations = ..., # maximum of 50 iterations for intermediate teams!\n",
    "           acquisition_function = ..., \n",
    "           objective_func = obj_func, #please have objective_func as input and do not change the 'obj_func' variable name!\n",
    "           acquisition_hyperparam= [...],\n",
    "           ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc38a9-6509-40a2-b6ef-f88a291f777b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
